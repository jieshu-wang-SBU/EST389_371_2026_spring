{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Read and Write Files using Pandas\n",
    "\n",
    "## Read csv files\n",
    "\n",
    "- Relative path: are partial and depend on the current working directory\n",
    "    - A single dot (./ or just the filename) refers to the current working directory.\n",
    "    - Two dots (../) move one level up in the directory hierarchy.\n",
    "    - Directory names separated by forward slashes (/) navigate into subfolders\n",
    "    - Sometimes, depending on your IDE, you might want to do extra configuration to be able to use relative path. For example, in the case of PyCharm, you need to mark the current working directory as _Sources Root_.\n",
    "- Absolute path: are complete and fixed"
   ],
   "id": "c6a91b871a8e0702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_selling_books = pd.read_csv('data/data_reading_path/best_selling_books_2023_2025.csv')\n",
    "\n",
    "print(\"DataFrame loaded from CSV:\\n\")\n",
    "best_selling_books.head()"
   ],
   "id": "8c4aa064db89059d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Some of the parameters you can pass\n",
    "\n",
    "best_selling_books = pd.read_csv('data/data_reading_path/best_selling_books_2023_2025.csv',\n",
    "                                 # usecols: Subset of columns to select\n",
    "                                 usecols=['Book name','Author','reviews count'],\n",
    "                                 # dtype: dtype or dictionary of dtypes\n",
    "                                 dtype={'Book name': str, 'reviews count': int},\n",
    "                                 # nrows: number of rows to read. I would read a small chunk just to inspect data if the file is large\n",
    "                                 nrows=10,\n",
    "                                 )\n",
    "\n",
    "print(\"DataFrame loaded from CSV:\\n\")\n",
    "best_selling_books.head()\n"
   ],
   "id": "353484743fb21361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d21e0ecf22708aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read Excel files\n",
    "\n",
    "Before you read an Excel file, make sure openpyxl package is installed. openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files."
   ],
   "id": "c258e034913448fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "tools = pd.read_excel('data/data_reading_path/ONET_data_tools_used.xlsx',\n",
    "                      sheet_name=0, # default 0, or a str of the name, or a list of the names\n",
    "                      )\n",
    "\n",
    "print(\"DataFrame loaded from xlsx files:\\n\")\n",
    "tools.head()\n"
   ],
   "id": "f6f823dd46e2f1c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you don't know the sheet names and wanted to inspect them, you could do the following:\n",
    "import pandas as pd\n",
    "\n",
    "file_to_check = pd.ExcelFile('data/data_reading_path/ONET_data_tools_used.xlsx')\n",
    "\n",
    "print(file_to_check.sheet_names)"
   ],
   "id": "a0e00b2e38989523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2aacb61549d437f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c4014a56d8f70e87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read JSON files\n",
    "JSON (JavaScript Object Notation) is a lightweight, text-based, language-independent data-interchange format designed to be human-readable and machine-parsable."
   ],
   "id": "3840793976d96503"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read the json file\n",
    "\n",
    "iris = pd.read_json('data/data_reading_path/iris.json')\n",
    "\n",
    "print(\"DataFrame loaded from JSON:\\n\")\n",
    "print(iris.head())"
   ],
   "id": "c8a8bd9fabed2829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6def18e159f062d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pickling\n",
    "Pickling can be useful to preserve Python obejects\n",
    "\n",
    "### Example 1: pickling of basic Python objects"
   ],
   "id": "25f9676f66dd5437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame where some cells contain lists and dictionaries\n",
    "# This is common when working with JSON, APIs, or semi-structured data\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"species\": [\"Sparrow\", \"Robin\", \"Blue Jay\"],\n",
    "\n",
    "    # Each cell in this column contains a LIST\n",
    "    \"observed_colors\": [\n",
    "        [\"brown\", \"gray\"],\n",
    "        [\"red\", \"brown\"],\n",
    "        [\"blue\", \"white\", \"black\"]\n",
    "    ],\n",
    "\n",
    "    # Each cell in this column contains a DICTIONARY\n",
    "    \"measurements\": [\n",
    "        {\"weight_g\": 24, \"wingspan_cm\": 20},\n",
    "        {\"weight_g\": 77, \"wingspan_cm\": 31},\n",
    "        {\"weight_g\": 100, \"wingspan_cm\": 43}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"DataFrame with lists and dictionaries in cells:\")\n",
    "print(df)\n"
   ],
   "id": "3c8dfadee9a243e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save as a pickle file and a csv file\n",
    "\n",
    "df.to_pickle('data/data_writing_path/birds_df.pkl')\n",
    "df.to_csv('data/data_writing_path/birds_df.csv', index=False) # index=False prevent creating a new index column\n",
    "print(\"DataFrame saved to pickle and csv.\\n\")\n"
   ],
   "id": "f65a0f492afd15ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's read the csv file and the pickle file\n",
    "\n",
    "birds_df_csv = pd.read_csv('data/data_writing_path/birds_df.csv')\n",
    "print(\"DataFrame loaded from csv:\")\n",
    "print(birds_df_csv)\n",
    "\n",
    "birds_df_pickled = pd.read_pickle('data/data_writing_path/birds_df.pkl')\n",
    "print(\"\\nDataFrame loaded from pickle:\")\n",
    "print(birds_df_pickled)\n"
   ],
   "id": "a1d4436e09207420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'The original dataframe contains list and dictionary columns, for example,\\n'\n",
    "      f'sparrow\\'s observed colors is: {(colors:=(df[df[\"species\"] == \"Sparrow\"][\"observed_colors\"].iloc[0]))}, and the type is {type(colors)}.')\n",
    "print(f\"\\nHowever, the dataframe we read back from the CSV file, sparrow's observed color is \"\n",
    "      f\"{(new_colors:=(birds_df_csv[birds_df_csv['species'] == 'Sparrow']['observed_colors'].iloc[0]))}, and the type is {type(new_colors)}.\")\n",
    "\n",
    "print(f\"\\nOn the other hand, in the dataframe we read back from the pickle file, it preserves the types. The sparrow's colors include {(pickle_colors:=(birds_df_pickled[birds_df_pickled['species']=='Sparrow']['observed_colors'].iloc[0]))}, and the type is {type(pickle_colors)}.\")"
   ],
   "id": "3446e045973d2f3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you have columns in your CSV file that are supposed to be Python types other than integer, float, or string, you can use ast.literal_eval. ast stands for Abstract Syntax Tree\n",
    "import ast\n",
    "\n",
    "literal_evaled_df = pd.read_csv('data/data_writing_path/birds_df.csv')\n",
    "\n",
    "# Convert the string column to a list column using .apply()\n",
    "literal_evaled_df['observed_colors'] = literal_evaled_df['observed_colors'].apply(ast.literal_eval)\n",
    "literal_evaled_df['measurements'] = literal_evaled_df['measurements'].apply(ast.literal_eval)\n"
   ],
   "id": "5b448c48ad9cd6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(type(literal_evaled_df['observed_colors'].iloc[0]))",
   "id": "4aa2e69bed7bf270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c6137fdbf86a8e06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ebd2341166a854b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "edf95c2f2722c977",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read only part of the file\n",
    "If your data file is large, you can use `nrows` to control how many rows you read as a dataframe"
   ],
   "id": "f2bc1e80a9c97ddf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chocolate_df = pd.read_csv('data/data_reading_path/chocolate_sales.csv', nrows=2)\n",
    "\n",
    "print(chocolate_df)"
   ],
   "id": "d2efa9fe03652093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b533e223cef0150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chunking\n",
    "To read a large file in chunks with pandas, you use the `chunksize` parameter in a file reading function like `pd.read_csv()`. This returns an iterator that yields smaller DataFrame objects (chunks) one by one, allowing you to process the data with limited memory.\n",
    "\n",
    "For example, you can read the file chunk by chunk and conduct some operations chunk by chunk, which takes much smaller memories."
   ],
   "id": "b5ad79ddb3286777"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 500\n",
    "\n",
    "chunks = []\n",
    "\n",
    "counter = 0\n",
    "for chunk in pd.read_csv('data/data_reading_path/chocolate_sales.csv',\n",
    "                         chunksize=chunk_size):\n",
    "    print(f\"Chunk {counter} index: {chunk.index.min()} ~ {chunk.index.max()}, shape {chunk.shape}\")\n",
    "    # You can conduct some operations by chunk\n",
    "    counter += 1\n",
    "    chunks.append(chunk)\n",
    "\n",
    "chocolate_df = pd.concat(chunks)\n",
    "\n",
    "print(\"\\nFinal dataframe:\")\n",
    "print(chocolate_df.head())"
   ],
   "id": "41ac962bd59fb38b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b715a6500d50387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a37ec9f0416021a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84bdb80c44f59de7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspect Data",
   "id": "1ab524206d566805"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/data_reading_path/chocolate_sales.csv')\n",
    "\n",
    "print(\"Preview first 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nPreview last 5 rows:\")\n",
    "print(df.tail())\n",
    "\n",
    "print(\"\\nBasic information about the DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nDataFrame shape (rows, columns):\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nEstimated memory usage:\")\n",
    "print(df.memory_usage(deep=True).sum(), \"bytes\")\n"
   ],
   "id": "fd7b5d81d5a1b92c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c7b6bc0cf6e7d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Writing data to files\n",
   "id": "78c317ffc9bed3a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "save_path = \"data/data_writing_path\"\n",
    "\n",
    "# Create a simple DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 32, 29],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"salary\": [70000, 85000, 78000]\n",
    "})\n",
    "\n",
    "print(df)"
   ],
   "id": "681ef31d388dcda7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write to CSV\n",
    "df.to_csv(f\"{save_path}/employees.csv\", index=False)\n"
   ],
   "id": "c634363716b98ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write to Excel\n",
    "# Again, it requires openpyxl package (usually already installed)\n",
    "\n",
    "df.to_excel(f\"{save_path}/employees.xlsx\", index=False)\n"
   ],
   "id": "a40070c3a729c6bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write to JSON\n",
    "# orient=\"records\" makes a list of dictionaries (very common in APIs)\n",
    "# indent=2 makes it human-readable\n",
    "\n",
    "df.to_json(f\"{save_path}/employees.json\", orient=\"records\", indent=2)\n"
   ],
   "id": "40b0bb7602fa097f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write to Pickle\n",
    "df.to_pickle(f\"{save_path}/employees.pkl\")\n"
   ],
   "id": "41faea7fdf2833de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "files = [\n",
    "    f\"{save_path}/employees.csv\",\n",
    "    f\"{save_path}/employees.xlsx\",\n",
    "    f\"{save_path}/employees.json\",\n",
    "    f\"{save_path}/employees.pkl\"\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    print(f\"{f}: {os.path.getsize(f)} bytes\")\n"
   ],
   "id": "c11c39a91c76451b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e43a11febdabecb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# [Optional] Polars\n",
    "\n",
    "Polars (linked here) is an open-source library for data manipulation, known for being one the fastest data processing solution on a single machine.\n"
   ],
   "id": "c28d500fd32baa72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n = 5_000_000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    \"user_id\": np.random.randint(1, 100_000, n),\n",
    "    \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\"], n),\n",
    "    \"price\": np.random.gamma(2, 50, n),\n",
    "    \"quantity\": np.random.randint(1, 5, n),\n",
    "    \"discount\": np.random.rand(n)\n",
    "}\n",
    "\n",
    "df_pd = pd.DataFrame(data)\n",
    "df_pl = pl.DataFrame(data)\n",
    "\n",
    "print(\"Pandas shape:\", df_pd.shape)\n",
    "print(\"Polars shape:\", df_pl.shape)\n"
   ],
   "id": "970de3504b698ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's do a series of computation:\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_pd[\"total\"] = df_pd[\"price\"] * df_pd[\"quantity\"]\n",
    "df_pd[\"revenue\"] = df_pd[\"total\"] * (1 - df_pd[\"discount\"])\n",
    "\n",
    "result_pd = (\n",
    "    df_pd\n",
    "    .groupby([\"user_id\", \"category\"])\n",
    "    .agg(\n",
    "        total_revenue=(\"revenue\", \"sum\"),\n",
    "        avg_price=(\"price\", \"mean\"),\n",
    "        total_quantity=(\"quantity\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Pandas pipeline time:\", end - start)\n"
   ],
   "id": "762dc23fb14822be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "result_pl = (\n",
    "    df_pl\n",
    "    .with_columns([\n",
    "        (pl.col(\"price\") * pl.col(\"quantity\")).alias(\"total\"),\n",
    "        (pl.col(\"price\") * pl.col(\"quantity\") * (1 - pl.col(\"discount\"))).alias(\"revenue\")\n",
    "    ])\n",
    "    .group_by([\"user_id\", \"category\"])\n",
    "    .agg([\n",
    "        pl.col(\"revenue\").sum().alias(\"total_revenue\"),\n",
    "        pl.col(\"price\").mean().alias(\"avg_price\"),\n",
    "        pl.col(\"quantity\").sum().alias(\"total_quantity\")\n",
    "    ])\n",
    "    .sort(\"total_revenue\", descending=True)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Polars eager pipeline time:\", end - start)\n"
   ],
   "id": "fa0e938cf49d1a8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "result_lazy = (\n",
    "    df_pl.lazy()\n",
    "    .with_columns([\n",
    "        (pl.col(\"price\") * pl.col(\"quantity\")).alias(\"total\"),\n",
    "        (pl.col(\"price\") * pl.col(\"quantity\") * (1 - pl.col(\"discount\"))).alias(\"revenue\")\n",
    "    ])\n",
    "    .group_by([\"user_id\", \"category\"])\n",
    "    .agg([\n",
    "        pl.col(\"revenue\").sum().alias(\"total_revenue\"),\n",
    "        pl.col(\"price\").mean().alias(\"avg_price\"),\n",
    "        pl.col(\"quantity\").sum().alias(\"total_quantity\")\n",
    "    ])\n",
    "    .sort(\"total_revenue\", descending=True)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Polars lazy pipeline time:\", end - start)\n"
   ],
   "id": "499dc81048681148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b85d0b33a155c0df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# [optional] tqdm - progress bar",
   "id": "b9bce717b15ec03f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Simulate a slow task\n",
    "for i in tqdm(range(20)):\n",
    "    time.sleep(0.15)"
   ],
   "id": "4c5f27a4109da68c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "for i in tqdm(\n",
    "    range(20),\n",
    "    desc=\"Downloading\",\n",
    "    total=20,\n",
    "    ncols=500,\n",
    "    unit=\"file\",\n",
    "    colour=\"blue\"\n",
    "):\n",
    "    time.sleep(0.15)\n"
   ],
   "id": "c1086fa15c622d50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create a larger DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"numbers\": np.random.randint(1, 100, size=100)\n",
    "})\n",
    "\n",
    "df.head()\n"
   ],
   "id": "76175e9493693fb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply Function WITHOUT tqdm\n",
    "\n",
    "def slow_square(x):\n",
    "    time.sleep(0.05)\n",
    "    return x ** 2\n",
    "\n",
    "df[\"squared\"] = df[\"numbers\"].apply(slow_square)\n"
   ],
   "id": "1ae639525069ec1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply Function WITH tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Dataframe for EST 389: \")  # Enable progress_apply\n",
    "\n",
    "df[\"squared\"] = df[\"numbers\"].progress_apply(slow_square)\n"
   ],
   "id": "ccfc5dc963527fa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36f21ce395426d25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c2bbb6adf6d2fa15",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
